---
title: 神经网络
date: 2020-05-19 21:25:29
tags: 机器学习
categories: 学习笔记
---

# 神经网络

![神经网络](https://bkimg.cdn.bcebos.com/pic/5366d0160924ab1803d0ea3038fae6cd7a890bf6?x-bce-process=image/watermark,g_7,image_d2F0ZXIvYmFpa2U4MA==,xp_5,yp_5)

## 组成

典型的人工神经网络具有以下三个部分：

**结构**（Architecture）结构指定了网络中的变量和它们的拓扑关系。例如，神经网络中的变量可以是神经元连接的权重（weights）和神经元的激励值（activities of the neurons）。  
**激励函数**（Activation Rule）大部分神经网络模型具有一个短时间尺度的动力学规则，来定义神经元如何根据其他神经元的活动来改变自己的激励值。一般激励函数依赖于网络中的权重（即该网络的参数）。  
**学习规则**（Learning Rule）学习规则指定了网络中的权重如何随着时间推进而调整。这一般被看做是一种长时间尺度的动力学规则。一般情况下，学习规则依赖于神经元的激励值。它也可能依赖于监督者提供的目标值和当前权重的值。例如，用于手写识别的一个神经网络，有一组输入神经元。输入神经元会被输入图像的数据所激发。在激励值被加权并通过一个函数（由网络的设计者确定）后，这些神经元的激励值被传递到其他神经元。这个过程不断重复，直到输出神经元被激发。最后，输出神经元的激励值决定了识别出来的是哪个字母。  
                        --------[Wikipedia](https://zh.wikipedia.org/wiki/%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C#%E5%88%86%E7%B1%BB)

## 激活函数(Activation Rule)

激活函数的一般性质:
可微，非线性，计算简单，单调，输出值有一定范围，归一化。

归一化：把数据映射到(0,1)范围内处理。

主要有三种：  
Sigmoid 函数,Tanh 函数(双曲正切),Relu函数

### Sigmoid 函数

表达式为
![Sigmoid](神经网络/sigmoid.png)

它长这样  
![xxx](https://bkimg.cdn.bcebos.com/pic/d009b3de9c82d158dfb4e7218a0a19d8bc3e426f?x-bce-process=image/watermark,g_7,image_d2F0ZXIvYmFpa2U4MA==,xp_1,yp_1)

一些性质：类似于生物学上的S型函数，关于(0,0.5)中心对称，两边过于平坦，处处可微。值域(0,1)。在边界处于饱和状态。

缺点：会有梯度弥散。不根据原点对称。计算耗时大。

梯度弥散：使用反向传播算法传播梯度的时候,随着传播深度的增加,梯度的幅度会急剧减小,会导致浅层神经元的权重更新非常缓慢,不能有效学习。
### Tanh 函数
![Tanh](神经网络/tanh.png)

![xx](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1587027878949&di=784457b00c9c6dfa44d34630a086a75d&imgtype=0&src=http%3A%2F%2Fpic2.zhimg.com%2Fv2-eec4407568e4260a50cb01aad2cdb24d_b.jpg)

一些性质：关于(0,0)中心对称，两边平滑。


优点：相对于前者，关于原点对称，计算较快。
缺点：仍未改变梯度弥散问题。
### Relu 函数

{% asset_img relu.png This is an example image %}

![Relu](神经网络/relu.png)

单侧抑制，有一个阈值，超过了这个阈值就激活，小于阈值就是0。小于阈值的话不可导。计算更快，稀疏激活性更大（导数再正值区为常数，负值区为0。）
![x](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1587027901095&di=5c21df48cfda6cb0bb64a2c7a1b2299c&imgtype=0&src=http%3A%2F%2Fc.biancheng.net%2Fuploads%2Fallimg%2F190108%2F2-1Z10QA3035F.gif)

优缺点：解决了部分梯度弥散问题，收敛速度更快。但仍未完全解决梯度弥散问题，然后在负区神经元die且不可复活。
由于反向传播等问题，升级了Relu函数。

### leaky Relu 函数

![leaky Relu](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1587033371072&di=2e81413bef5018c5358bc7486b8e135a&imgtype=0&src=http%3A%2F%2F5b0988e595225.cdn.sohucs.com%2Fimages%2F20171103%2Fb9f1d55cf7914583ad66ca5bec01f9e1.png)

解决了神经元die的问题。

(未完待续)