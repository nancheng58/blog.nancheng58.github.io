<!DOCTYPE html><html lang="zh-cn"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="nancheng58"><meta name="copyright" content="nancheng58"><meta name="generator" content="Hexo 5.4.0"><meta name="theme" content="hexo-theme-yun"><title>一些回归任务中的损失函数 | nancheng58的小站</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@900&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.25/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_dxory92pb0h.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>function initScrollReveal() {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
}
document.addEventListener("DOMContentLoaded", initScrollReveal);
document.addEventListener("pjax:success", initScrollReveal);
</script><link rel="icon" href="/nancheng58.ico"><link rel="mask-icon" href="/nancheng58.ico" color="#0078E7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"hostname":"nancheng58.github.io","root":"/","title":"Nancheng58的小站","version":"1.6.2","mode":"auto","copycode":true,"page":{"isPost":true},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}.","hits":"${hits} results found","hits_time":"${hits} results found in ${time} ms"},"anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"local_search":{"path":"/search.xml"},"fireworks":{"colors":["102, 167, 221","62, 131, 225","33, 78, 194"]}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/utils.js"></script><script src="/js/hexo-theme-yun.js"></script><link rel="alternate" href="/atom.xml" title="nancheng58的小站" type="application/atom+xml"><meta name="description" content="Regression Loss Function    本文整理了回归任务中常用的一些损失函数及其对比。  均方误差  均方误差(MSE)是最常用的回归损失函数，计算方法是求预测值与真实值之间距离的平方和，公式如图。">
<meta property="og:type" content="article">
<meta property="og:title" content="一些回归任务中的损失函数">
<meta property="og:url" content="http://nancheng58.github.io/2021/loss%20function/index.html">
<meta property="og:site_name" content="nancheng58的小站">
<meta property="og:description" content="Regression Loss Function    本文整理了回归任务中常用的一些损失函数及其对比。  均方误差  均方误差(MSE)是最常用的回归损失函数，计算方法是求预测值与真实值之间距离的平方和，公式如图。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/33d3c13e-f118-4911-9d8d-440f70080e00/1529558773906.png">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/5b9c3dde-5e2a-4a82-a81a-fcc29ee1d37c/1529558774239.png">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/bd4624ba-f00c-42aa-b06d-374b74aae671/1529558773392.png">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/12342b51-bd31-4d68-88ce-f45a616f1192/1529558773472.png">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/6d4dda44-9709-4529-b61b-715501daee14/1529558773554.png">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/8df05f5c-8b9d-4eb2-85c9-88209dd95582/1529558777063.png">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/561e9469-150f-40dc-9586-0be6ec7483df/1529558774147.png">
<meta property="og:image" content="https://img2018.cnblogs.com/blog/439761/201912/439761-20191217152559345-962121282.gif">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/196f302b-e3dc-4761-8cbc-26955519dbe5/1529558774452.png">
<meta property="og:image" content="http://nancheng58.github.io/2021/11/01/loss%20function/2021-10-31-11-35-02.png">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/74081c19-c02f-420b-9655-54ea4bededc9/1529558774637.png">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/c9671684-85bf-4a7c-9b7d-9c40ebe37eec/1529558776657.png">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/f0d9e9d2-6e32-4bf6-97b7-0358ce8a6eaa/1529558776580.png">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/25f955e0-efd4-48db-b78c-eee6eb0015a8/1529558777233.png">
<meta property="og:image" content="https://image.jiqizhixin.com/uploads/editor/75dca5f7-03ff-4ff7-bdaa-7cc61ccc35ed/1529558777413.png">
<meta property="article:published_time" content="2021-11-01T14:35:29.000Z">
<meta property="article:modified_time" content="2021-11-01T05:51:35.103Z">
<meta property="article:author" content="nancheng58">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://image.jiqizhixin.com/uploads/editor/33d3c13e-f118-4911-9d8d-440f70080e00/1529558773906.png"><script src="/js/ui/mode.js"></script><link rel="stylesheet" href="/css/prism.css" type="text/css"></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="Table of Contents"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="Overview"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info mickey-mouse"><a class="site-author-avatar" href="/about/" title="nancheng58"><img width="96" loading="lazy" src="/images/nancheng58.jpg" alt="nancheng58"><span class="site-author-status" title="嘻嘻嘻嘻嘻">😊</span></a><div class="site-author-name"><a href="/about/">nancheng58</a></div><a class="site-name" href="/about/site.html">nancheng58的小站</a><sub class="site-subtitle"></sub><div class="site-desciption">一个普通人.</div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="Home"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="Archives"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">9</span></a></div><div class="site-state-item"><a href="/categories/" title="Categories"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">4</span></a></div><div class="site-state-item"><a href="/tags/" title="Tags"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">6</span></a></div><a class="site-state-item hty-icon-button" href="/links/" title="友链"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-settings-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="/atom.xml" title="RSS" target="_blank" style="color:orange"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-rss-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/nancheng58" title="GitHub" target="_blank" style="color:#181717"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:nancheng58@qq.coms" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="我的小伙伴们" style="color:dodgerblue"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-genderless-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Regression-Loss-Function"><span class="toc-number">1.</span> <span class="toc-text">Regression Loss Function</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.0.1.</span> <span class="toc-text">均方误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E5%80%BC%E8%AF%AF%E5%B7%AE%EF%BC%88%E4%B9%9F%E7%A7%B0L1%E6%8D%9F%E5%A4%B1%EF%BC%89"><span class="toc-number">1.0.2.</span> <span class="toc-text">平均绝对值误差（也称L1损失）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MSE%EF%BC%88L2%E6%8D%9F%E5%A4%B1%EF%BC%89%E4%B8%8EMAE%EF%BC%88L1%E6%8D%9F%E5%A4%B1%EF%BC%89%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number">1.0.3.</span> <span class="toc-text">MSE（L2损失）与MAE（L1损失）的比较</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%8E%E5%9B%BE%E4%B8%AD%E5%8F%AF%E4%BB%A5%E7%9F%A5%E9%81%93%E4%BB%80%E4%B9%88%EF%BC%9F%E5%BA%94%E5%BD%93%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-number">2.</span> <span class="toc-text">从图中可以知道什么？应当如何选择损失函数？</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B9%E6%8D%AE%E4%B8%8D%E5%90%8C%E6%83%85%E5%86%B5%E9%80%89%E6%8B%A9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text">根据不同情况选择损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Huber%E6%8D%9F%E5%A4%B1%EF%BC%8C%E5%B9%B3%E6%BB%91%E7%9A%84%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">2.2.</span> <span class="toc-text">Huber损失，平滑的平均绝对误差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BD%95%E8%A6%81%E4%BD%BF%E7%94%A8Huber%E6%8D%9F%E5%A4%B1%EF%BC%9F"><span class="toc-number">2.3.</span> <span class="toc-text">为何要使用Huber损失？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Log-Cosh%E6%8D%9F%E5%A4%B1"><span class="toc-number">2.4.</span> <span class="toc-text">Log-Cosh损失</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E4%BD%8D%E6%95%B0%E6%8D%9F%E5%A4%B1-Quantile-Loss"><span class="toc-number">2.4.1.</span> <span class="toc-text">分位数损失(Quantile Loss)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%90%86%E8%A7%A3%E5%88%86%E4%BD%8D%E6%95%B0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.4.2.</span> <span class="toc-text">理解分位数损失函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E7%A0%94%E7%A9%B6"><span class="toc-number">2.5.</span> <span class="toc-text">对比研究</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%BF%E7%9C%9F%E5%AF%B9%E6%AF%94%E7%9A%84%E4%B8%80%E4%BA%9B%E8%A7%82%E5%AF%9F%E7%BB%93%E6%9E%9C%EF%BC%9A"><span class="toc-number">2.6.</span> <span class="toc-text">仿真对比的一些观察结果：</span></a></li></ol></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="http://nancheng58.github.io/2021/loss%20function/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="nancheng58"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="nancheng58的小站"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">一些回归任务中的损失函数<a class="post-edit-link" href="https://github.com/nancheng58/nancheng58.github.io/tree/master/_posts/loss function.md" target="_blank" title="Edit this post" rel="noopener"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-edit-line"></use></svg></a></h1><div class="post-meta"><div class="post-time" style="display:inline-block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <time title="Created: 2021-11-01 22:35:29" itemprop="dateCreated datePublished" datetime="2021-11-01T22:35:29+08:00">2021-11-01</time></div><span class="post-busuanzi"><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="Views"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span id="busuanzi_value_page_pv"></span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category-item" href="/categories/Slide/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">Slide</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag-item" href="/tags/Machine-Learning/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">Machine Learning</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#0078E7;"><blockquote>
<h1 id="Regression-Loss-Function"><a href="#Regression-Loss-Function" class="headerlink" title="Regression Loss Function"></a><em>Regression Loss Function</em></h1></blockquote>
<!-- _class: lead gaia -->
<p>  本文整理了回归任务中常用的一些损失函数及其对比。</p>
<blockquote>
<h3 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a><strong>均方误差</strong></h3></blockquote>
<p><img src="https://image.jiqizhixin.com/uploads/editor/33d3c13e-f118-4911-9d8d-440f70080e00/1529558773906.png" alt="w:250" loading="lazy"></p>
<p>均方误差(MSE)是最常用的回归损失函数，计算方法是求预测值与真实值之间距离的平方和，公式如图。</p>
<span id="more"></span>
<p>下图是MSE函数的图像，其中目标值是100，预测值的范围从-10000到10000，Y轴代表的MSE取值范围是从0到正无穷，并且在预测值为100处达到最小。</p>
<p><img src="https://image.jiqizhixin.com/uploads/editor/5b9c3dde-5e2a-4a82-a81a-fcc29ee1d37c/1529558774239.png" alt="w:510" loading="lazy">MSE损失（Y轴）-预测值（X轴)</p>
<hr>
<blockquote>
<h3 id="平均绝对值误差（也称L1损失）"><a href="#平均绝对值误差（也称L1损失）" class="headerlink" title="平均绝对值误差（也称L1损失）"></a><strong>平均绝对值误差（也称L1损失）</strong></h3></blockquote>
<p><img src="https://image.jiqizhixin.com/uploads/editor/bd4624ba-f00c-42aa-b06d-374b74aae671/1529558773392.png" alt="w:250" loading="lazy"></p>
<p>平均绝对误差（MAE）是另一种用于回归模型的损失函数。MAE是目标值和预测值之差的绝对值之和。其只衡量了预测值误差的平均模长，而不考虑方向，取值范围也是从0到正无穷（如果考虑方向，则是残差/误差的总和——平均偏差（MBE））。</p>
<p><img src="https://image.jiqizhixin.com/uploads/editor/12342b51-bd31-4d68-88ce-f45a616f1192/1529558773472.png" alt="w:520" loading="lazy"></p>
<p>MAE损失（Y轴）-预测值（X轴）</p>
<hr>
<blockquote>
<h3 id="MSE（L2损失）与MAE（L1损失）的比较"><a href="#MSE（L2损失）与MAE（L1损失）的比较" class="headerlink" title="MSE（L2损失）与MAE（L1损失）的比较"></a><strong>MSE（L2损失）与MAE（L1损失）的比较</strong></h3></blockquote>
<p>简单来说，MSE计算简便，但MAE对异常点有更好的鲁棒性。下面介绍导致二者差异的原因。</p>
<p>下面让我们观察MAE和RMSE（即MSE的平方根，同MAE在同一量级中）在两个例子中的计算结果。第一个例子中，预测值和真实值很接近，而且误差的方差也较小。第二个例子中，因为存在一个异常点，而导致误差非常大。</p>
<p><img src="https://image.jiqizhixin.com/uploads/editor/6d4dda44-9709-4529-b61b-715501daee14/1529558773554.png" alt="img" loading="lazy"></p>
<p>左图：误差比较接近 右图：有一个误差远大于其他误差</p>
<hr>
<blockquote>
<h1 id="从图中可以知道什么？应当如何选择损失函数？"><a href="#从图中可以知道什么？应当如何选择损失函数？" class="headerlink" title="从图中可以知道什么？应当如何选择损失函数？"></a><strong>从图中可以知道什么？应当如何选择损失函数？</strong></h1></blockquote>
<p>MSE对误差取了平方（令E=真实值-预测值），因此若E&gt;1，则MSE会进一步增大误差。如果数据中存在异常点，那么e值就会很大，而E²则会远大于|E|。</p>
<p>因此，相对于使用MAE计算损失，使用MSE的模型会赋予异常点更大的权重。在例子中，用RMSE计算损失的模型会以牺牲了其他样本的误差为代价，朝着减小异常点误差的方向更新。然而这就会降低模型的整体性能。</p>
<p>如果训练数据被异常点所污染，那么MAE损失就更好用（比如，在训练数据中存在大量错误的反例和正例标记，但是在测试集中没有这个问题）。</p>
<p>然而MAE存在一个严重的问题（特别是对于神经网络）：更新的梯度始终相同，也就是说，即使对于很小的损失值，梯度也很大。这样不利于模型的学习。为了解决这个缺陷，我们可以使用变化的学习率，在损失接近最小值时降低学习率。</p>
<p>而MSE在这种情况下的表现就很好，即便使用固定的学习率也可以有效收敛。MSE损失的梯度随损失增大而增大，而损失趋于0时则会减小。这使得在训练结束时，使用MSE模型的结果会更精确。</p>
<p><img src="https://image.jiqizhixin.com/uploads/editor/8df05f5c-8b9d-4eb2-85c9-88209dd95582/1529558777063.png" alt="w:750" loading="lazy"></p>
<hr>
<blockquote>
<h2 id="根据不同情况选择损失函数"><a href="#根据不同情况选择损失函数" class="headerlink" title="根据不同情况选择损失函数"></a><strong>根据不同情况选择损失函数</strong></h2></blockquote>
<p>如果异常点代表在商业中很重要的异常情况，并且需要被检测出来，则应选用MSE损失函数。相反，如果只把异常值当作受损数据，则应选用MAE损失函数。</p>
<p>总而言之，处理异常点时，L1损失函数更稳定，但它的导数不连续，因此求解效率较低。L2损失函数对异常点更敏感，但通过令其导数为0，可以得到更稳定的封闭解。</p>
<p>二者兼有的问题是：在某些情况下，上述两种损失函数都不能满足需求。例如，若数据中90%的样本对应的目标值为150，剩下10%在0到30之间。那么使用MAE作为损失函数的模型可能会忽视10%的异常点，而对所有样本的预测值都为150。</p>
<p>这是因为模型会按中位数来预测。而使用MSE的模型则会给出很多介于0到30的预测值，因为模型会向异常点偏移。上述两种结果在许多商业场景中都是不可取的。</p>
<p>这些情况下应该怎么办呢？最简单的办法是对目标变量进行变换。而另一种办法则是换一个损失函数，这就引出了下面要讲的第三种损失函数，即Huber损失函数。</p>
<hr>
<h2 id="Huber损失，平滑的平均绝对误差"><a href="#Huber损失，平滑的平均绝对误差" class="headerlink" title="Huber损失，平滑的平均绝对误差"></a><strong>Huber损失，平滑的平均绝对误差</strong></h2><p>Huber损失对数据中的异常点没有平方误差损失那么敏感。它在0也可微分。本质上，Huber损失是绝对误差，只是在误差很小时，就变为平方误差。误差降到多小时变为二次误差由超参数δ（delta）来控制。当Huber损失在[0-δ,0+δ]之间时，等价为MSE，而在[-∞,δ]和[δ,+∞]时为MAE。</p>
<p><img src="https://image.jiqizhixin.com/uploads/editor/561e9469-150f-40dc-9586-0be6ec7483df/1529558774147.png" alt="w:550" loading="lazy"></p>
<hr>
<blockquote>
<h2 id="为何要使用Huber损失？"><a href="#为何要使用Huber损失？" class="headerlink" title="为何要使用Huber损失？"></a><strong>为何要使用Huber损失？</strong></h2></blockquote>
<p>使用MAE训练神经网络最大的一个问题就是不变的大梯度，这可能导致在使用梯度下降快要结束时，错过了最小点。而对于MSE，梯度会随着损失的减小而减小，使结果更加精确。</p>
<p>在这种情况下，Huber损失就非常有用。它会由于梯度的减小而落在最小值附近。比起MSE，它对异常点更加鲁棒。因此，Huber损失结合了MSE和MAE的优点。但是，Huber损失的问题是我们可能需要不断调整超参数delta。</p>
<p><img src="https://img2018.cnblogs.com/blog/439761/201912/439761-20191217152559345-962121282.gif" alt="w:500" loading="lazy"><br>这里超参数delta的选择非常重要，因为这决定了对异常点的定义。当残差大于delta，应当采用L1（对较大的异常值不那么敏感）来最小化，而残差小于超参数，则用L2来最小化。</p>
<hr>
<h2 id="Log-Cosh损失"><a href="#Log-Cosh损失" class="headerlink" title="Log-Cosh损失"></a><strong>Log-Cosh损失</strong></h2><p>Log-cosh是另一种应用于回归问题中的，且比L2更平滑的的损失函数。它的计算方式是预测误差的双曲余弦的对数。<br><img src="https://image.jiqizhixin.com/uploads/editor/196f302b-e3dc-4761-8cbc-26955519dbe5/1529558774452.png" alt="w:300" loading="lazy"><br><img src="/2021/11/01/loss%20function/2021-10-31-11-35-02.png" alt="w:200" loading="lazy"><br><img src="https://image.jiqizhixin.com/uploads/editor/74081c19-c02f-420b-9655-54ea4bededc9/1529558774637.png" alt="w:300" loading="lazy"></p>
<p>优点：（1）对于较小的x，log(cosh(x))近似等于(x^2)/2，对于较大的x，近似等于abs(x)-log(2)。这意味着‘logcosh’基本类似于均方误差，但不易受到异常点的影响。（2）Log-Cosh处处二阶可微，例如XGBoost，采用牛顿法来寻找最优点，需要求解二阶导数（Hessian），损失函数的二阶可微是很有必要的。<br>缺点：相比于Huber,Log-Cosh求导比较复杂，计算量较大，在深度学习中使用不多。而且其存在某些问题。比如误差很大，一阶梯度和Hessian会变成定值，这就导致XGBoost出现缺少分裂点的情况。</p>
<hr>
<blockquote>
<h3 id="分位数损失-Quantile-Loss"><a href="#分位数损失-Quantile-Loss" class="headerlink" title="分位数损失(Quantile Loss)"></a><strong>分位数损失(Quantile Loss)</strong></h3></blockquote>
<p>通常的回归算法是拟合训练数据的期望或者中位数，而使用分位数损失函数可以通过给定不同的分位点，拟合训练数据的不同分位数。</p>
<p>$L_{quantile} = \frac{1}{N}\sum_{i=1}^N \amalg_{y &gt; f(x)}(1-\gamma)\mid y-f(x)\mid + \amalg_{y &lt; f(x)}\gamma \mid y - f(x) \mid$</p>
<p>该函数是一个分段函数，γ 为分位数系数，y为真实值，f(x)为预测值。根据预测值和真实值的大小，分两种情况来开考虑。y&gt;f(x) 为高估，预测值比真实值大；y&lt;f(x)为低估，预测值比真实值小，使用不同过得系数来控制高估和低估在整个损失值的权重 。</p>
<p>特别的，当γ=0.5 时，分位数损失退化为平均绝对误差MAE，也可以将MAE看成是分位数损失的一个特例 - 中位数损失。</p>
<p>当我们更关注区间预测而不仅是点预测时，分位数损失函数就很有用。使用最小二乘回归进行区间预测，基于的假设是残差（y-y_hat）是独立变量，且方差保持不变。</p>
<p>一旦违背了这条假设，那么线性回归模型就不成立。但是我们也不能因此就认为使用非线性函数或基于树的模型更好，而放弃将线性回归模型作为基线方法。这时，分位数损失和分位数回归就派上用场了，因为即便对于具有变化方差或非正态分布的残差，基于分位数损失的回归也能给出合理的预测区间。</p>
<hr>
<blockquote>
<h3 id="理解分位数损失函数"><a href="#理解分位数损失函数" class="headerlink" title="理解分位数损失函数"></a><strong>理解分位数损失函数</strong></h3></blockquote>
<p>如何选取合适的分位值取决于我们对正误差和反误差的重视程度。损失函数通过分位值（γ）对高估和低估给予不同的惩罚。例如，当分位数损失函数γ=0.25时，对高估的惩罚更大，使得预测值略低于中值。</p>
<p>γ是所需的分位数，其值介于0和1之间。</p>
<p><img src="https://image.jiqizhixin.com/uploads/editor/c9671684-85bf-4a7c-9b7d-9c40ebe37eec/1529558776657.png" alt="w:650" loading="lazy">分位数损失（Y轴）与预测值（X轴）图示。Y的真值为0</p>
<hr>
<p>这个损失函数也可以在神经网络或基于树的模型中计算预测区间。以下是用Sklearn实现梯度提升树回归模型的示例。</p>
<p><img src="https://image.jiqizhixin.com/uploads/editor/f0d9e9d2-6e32-4bf6-97b7-0358ce8a6eaa/1529558776580.png" alt="w:720" loading="lazy">使用分位数损失（梯度提升回归器）预测区间</p>
<p>上图表明：在sklearn库的梯度提升回归中使用分位数损失可以得到90％的预测区间。其中上限为γ=0.95，下限为γ=0.05。</p>
<hr>
<blockquote>
<h2 id="对比研究"><a href="#对比研究" class="headerlink" title="对比研究"></a><strong>对比研究</strong></h2></blockquote>
<p>为了证明上述所有损失函数的特点，让我们来一起看一个对比研究。首先，建立一个从sinc（x）函数中采样得到的数据集，并引入了两项人为噪声：高斯噪声分量ε〜N（0，σ2）和脉冲噪声分量ξ〜Bern（p）。<br>加入脉冲噪声是为了说明模型的鲁棒效果。以下是使用不同损失函数拟合GBM回归器的结果。</p>
<p><img src="https://image.jiqizhixin.com/uploads/editor/25f955e0-efd4-48db-b78c-eee6eb0015a8/1529558777233.png" alt="w:900" loading="lazy"></p>
<p>连续损失函数：（A）MSE损失函数；（B）MAE损失函数；（C）Huber损失函数；（D）分位数损失函数。将一个平滑的GBM拟合成有噪声的sinc（x）数据的示例：（E）原始sinc（x）函数；（F）具有MSE和MAE损失的平滑GBM；（G）具有Huber损失的平滑GBM，且δ={4,2,1}；（H）具有分位数损失的平滑的GBM，且α={0.5,0.1,0.9}。</p>
<hr>
<h2 id="仿真对比的一些观察结果："><a href="#仿真对比的一些观察结果：" class="headerlink" title="仿真对比的一些观察结果："></a><strong>仿真对比的一些观察结果：</strong></h2><p>MAE损失模型的预测结果受脉冲噪声的影响较小，而MSE损失函数的预测结果受此影响略有偏移。<br>Huber损失模型预测结果对所选超参数不敏感。<br>分位数损失模型在合适的置信水平下能给出很好的估计。</p>
<p><img src="https://image.jiqizhixin.com/uploads/editor/75dca5f7-03ff-4ff7-bdaa-7cc61ccc35ed/1529558777413.png" alt="w:800" loading="lazy"></p>
</div><ul class="post-copyright"><li class="post-copyright-author"><strong>Post author: </strong>nancheng58</li><li class="post-copyright-link"><strong>Post link: </strong><a href="http://nancheng58.github.io/2021/loss%20function/" title="一些回归任务中的损失函数">http://nancheng58.github.io/2021/loss%20function/</a></li><li class="post-copyright-license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> unless otherwise stated.</li></ul></section></article><div class="post-nav"><div class="post-nav-item"></div><div class="post-nav-item"><a class="post-nav-next" href="/2021/conditional%20expectation/" rel="next" title="条件期望及重期望法则"><span class="post-nav-text">条件期望及重期望法则</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>若您无 GitHub 账号，可直接在下方匿名评论。</span><br><span>若您想及时得到回复提醒，建议跳转 GitHub Issues 评论。</span><br><span>下述comment采用了Valine，头像为邮箱相应的Gravatar头像。</span><br></div><div id="minivaline-container"></div><script>Yun.utils.getScript("https://cdn.jsdelivr.net/npm/minivaline@6", () => {
  const minivalineConfig = {"enable":true,"serverURL":"https://mini-valine-admin-api-vercel-zeta.vercel.app/","md":true,"backend":"waline"}
  minivalineConfig.el = "#minivaline-container"
  new MiniValine(minivalineConfig);
}, window.MiniValine);</script></div></main><footer class="sidebar-translate" id="footer"><div class="beian"><a rel="noopener" href="https://beian.miit.gov.cn/" target="_blank">鲁ICP备2021014892号</a></div><div class="copyright"><span>&copy; 2019 – 2021 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-cloud-line"></use></svg></span><span class="author"> nancheng58</span></div><div class="powered"><span>Powered by <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> v5.4.0</span><span class="footer-separator">|</span><span>Theme - <a rel="noopener" href="https://github.com/YunYouJun/hexo-theme-yun" target="_blank"><span>Yun</span></a> v1.6.2</span></div><div id="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv" title="Total Visitors"><span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-user-line"></use></svg></span><span id="busuanzi_value_site_uv"></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv" title="Total Views"><span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg></span><span id="busuanzi_value_site_pv"></span></span></div></footer><a class="hty-icon-button" id="back-to-top" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" target="_blank" rel="noopener" href="https://www.google.com/search?q=site:nancheng58.github.io" title="Search"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search-line"></use></svg></span></a></div></body></html>